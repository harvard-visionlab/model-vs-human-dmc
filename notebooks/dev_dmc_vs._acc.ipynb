{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelvshuman_dmc import constants as c\n",
    "from modelvshuman_dmc.plotting.plot import a, get_dataset_names, get_experiments, get_human_and_CNN_subjects\n",
    "from modelvshuman_dmc.helper import plotting_helper as ph\n",
    "from examples.simclr_hn_models import plotting_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_def_name = 'plotting_definition_alexnets_simclr_hn'\n",
    "plotting_definition = plotting_definitions.__dict__[plotting_def_name]\n",
    "plotting_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting_definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_types = c.DEFAULT_PLOT_TYPES\n",
    "plot_types = ['accuracy']\n",
    "plot_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot(plot_types = plot_types, plotting_definition = plotting_def, figure_directory_name = figure_dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = ['cue-conflict', 'edge', 'silhouette', 'sketch', 'stylized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_type = 'accuracy'\n",
    "current_dataset_names = get_dataset_names(plot_type)\n",
    "print(current_dataset_names)\n",
    "datasets = get_experiments(current_dataset_names)\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = a.SixteenClassAccuracy()\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_maker_fun = plotting_definition\n",
    "decision_maker_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_accuracy(datasets, decision_maker_fun, result_dir):\n",
    "#     plot_general_analyses(datasets=datasets, analysis=a.SixteenClassAccuracy(),\n",
    "#                           decision_maker_fun=decision_maker_fun,\n",
    "#                           result_dir=result_dir, plot_type=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,d in enumerate(datasets):\n",
    "    print(idx, d.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,d in enumerate(datasets):\n",
    "\n",
    "    df = ph.get_experimental_data(d)\n",
    "\n",
    "    for e in d.experiments:\n",
    "        decision_makers = decision_maker_fun(df)\n",
    "        result_df = analysis.get_result_df(df=df,\n",
    "                                           decision_makers=decision_makers,\n",
    "                                           experiment=e)\n",
    "        break\n",
    "    \n",
    "    if idx==11:\n",
    "        print(d)\n",
    "        break\n",
    "        # x_y_plot(figure_path=figure_path,\n",
    "        #          result_df=result_df,\n",
    "        #          decision_makers=decision_makers,\n",
    "        #          analysis=analysis,\n",
    "        #          experiment=e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.subj=='alexnet_anime_sgd_lr005'])\n",
    "cond1 = df[(df.subj=='alexnet_anime_sgd_lr005') & (df.condition=='cr')].sort_values(by=\"filename\")\n",
    "cond2 = df[(df.subj=='alexnet_anime_sgd_lr005') & (df.condition=='bw')].sort_values(by=\"filename\")\n",
    "len(cond1), len(cond2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = df.subj.unique()\n",
    "humans, dnns = get_human_and_CNN_subjects(subjects.tolist())\n",
    "print(humans)\n",
    "print(dnns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "conditions = df.condition.unique()\n",
    "cond = conditions[0]\n",
    "\n",
    "subj1 = 'alexnet_anime_sgd_lr005'\n",
    "subj2 = 'alexnet_lgn2_w1_mlp_simclrhn_probe1'\n",
    "# subj2 = 'clip'\n",
    "subset1 = df[(df.subj==subj1) & (df.condition==cond)].sort_values(by=\"filename\")\n",
    "subset2 = df[(df.subj==subj2) & (df.condition==cond)].sort_values(by=\"filename\")\n",
    "assert (subset1.filename==subset2.filename).all()\n",
    "values1 = subset1.decision_margin.values.squeeze()\n",
    "values2 = subset2.decision_margin.values.squeeze()\n",
    "pearsonr(values1, values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.scatterplot(x=values1, y=values2)\n",
    "ax.axis('square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelvshuman_dmc.helper.dmc import compute_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_consistency(subset1, subset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# analyze human data\n",
    "\n",
    "We're using the average percent correct across people to estimate the per-item decision margin. We need error bars / ceiling on the human data. We'll get that by correlating percent correct across halves, then using spearman brown to estimate the reliability of the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def spearman_brown(r):\n",
    "    '''prophecy formula (Brown, 1910; Spearman, 1910)'''\n",
    "    return (2*r)/(1+r)\n",
    "\n",
    "def get_split_halves(n):\n",
    "    subjects = set(range(n))\n",
    "    all_combos = list(itertools.combinations(subjects, n//2))\n",
    "    all_splits = [[tuple(combo), tuple(subjects - set(combo))] for combo in all_combos]\n",
    "    \n",
    "    # make sure we don't repeat splits (reliability calc is symmetric)\n",
    "    chunks = []\n",
    "    splits = []\n",
    "    for a,b in all_splits:\n",
    "        if a in chunks: continue\n",
    "        chunks.append(a)\n",
    "        chunks.append(b)\n",
    "        splits.append([a,b])\n",
    "    \n",
    "    for snum, split in enumerate(splits):    \n",
    "        #print(split)\n",
    "        assert subjects == set(split[0]+split[1]), f'Oops, split missing subjects {snum}'\n",
    "        \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import progress_bar\n",
    "\n",
    "def compute_reliability_from_halves(df):\n",
    "    subjects = df.subj.unique()\n",
    "    results = defaultdict(list)\n",
    "    N = len(subjects)\n",
    "    halves = get_split_halves(N)\n",
    "    for split_num,(splitA,splitB) in enumerate(progress_bar(halves)):\n",
    "        subsA = np.array(subjects)[np.array(splitA)]\n",
    "        subsB = np.array(subjects)[np.array(splitB)]\n",
    "\n",
    "        subsetA = df[df.subj.isin(subsA)]\n",
    "        subsetB = df[df.subj.isin(subsB)]\n",
    "\n",
    "        avgA = subsetA.groupby(by=['filename']).mean(numeric_only=True).reset_index()\n",
    "        avgB = subsetB.groupby(by=['filename']).mean(numeric_only=True).reset_index()\n",
    "        assert (avgA.filename == avgB.filename).all()\n",
    "        corr = pearsonr(avgA.is_correct, avgB.is_correct)[0]\n",
    "\n",
    "        results['split_num'].append(split_num)\n",
    "        results['splitA'].append(splitA)\n",
    "        results['splitB'].append(splitB)\n",
    "        results['pearsonr'].append(corr)\n",
    "        \n",
    "    human_split_corr = pd.DataFrame(results)\n",
    "    mean_corr = human_split_corr.pearsonr.mean()\n",
    "    reliability = spearman_brown(mean_corr)\n",
    "    \n",
    "    return human_split_corr, reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df = df[df.subj.isin(humans)].copy().reset_index()\n",
    "human_df.loc[:,'filename'] = human_df.imagename.apply(lambda x: \"_\".join(x.split(\"_\")[-2:]))\n",
    "human_df.loc[:,'is_correct'] = (human_df.object_response==human_df.category).astype(float)\n",
    "human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df = human_df.groupby(by=['filename']).mean(numeric_only=True).reset_index()\n",
    "human_df.insert(0, 'subj', 'human_avg')\n",
    "human_df = human_df.drop(columns=['index', 'session', 'trial', 'targ_act', 'max_nontarg_act'])\n",
    "human_df.loc[:,'decision_margin'] = human_df.loc[:,'is_correct']\n",
    "human_df = human_df.sort_values(by=\"filename\")\n",
    "human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_df = df[df.subj.isin(dnns)]\n",
    "dnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ = pd.concat([dnn_df, human_df])\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "results = defaultdict(list)\n",
    "subset1 = human_df\n",
    "values1 = subset1.decision_margin.values.squeeze()\n",
    "for dnn in dnns:\n",
    "    subset2 = df[df.subj==dnn].sort_values(by=\"filename\")\n",
    "    assert (subset1.filename.values==subset2.filename.values).all()    \n",
    "    values2 = subset2.decision_margin.values.squeeze()\n",
    "    corr = pearsonr(values1, values2)[0]\n",
    "    results['dnn'].append(dnn)\n",
    "    results['decision_margin_consistency'].append(corr)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"decision_margin_consistency\", ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelvsmodel_df = compute_consistency(dnn_df)\n",
    "modelvsmodel_df.decision_margin_consistency.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Adjust figure size\n",
    "plt.figure(figsize=(10, 6))  # Width: 10, Height: 6\n",
    "\n",
    "ax = sns.barplot(data=results_df, x=\"dnn\", y=\"decision_margin_consistency\")\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# redux, draft analysis\n",
    "\n",
    "extra = ['cue-conflict', 'edge', 'silhouette', 'sketch', 'stylized']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "root_dir = '../outputs/raw-data/'\n",
    "experiments = natsorted([Path(f.path).name for f in os.scandir(root_dir) if f.is_dir()])\n",
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'edge'\n",
    "experiment = 'sketch'\n",
    "experiment = 'silhouette'\n",
    "experiment = 'stylized'\n",
    "experiment = 'cue-conflict'\n",
    "experiment = 'colour'\n",
    "files = glob(os.path.join(root_dir, experiment, '*.csv'))\n",
    "human_files = natsorted([filename for filename in files if \"_subject-\" in filename])\n",
    "dnn_files = natsorted([filename for filename in files if \"_subject-\" not in filename])\n",
    "dnn_df = pd.concat([pd.read_csv(filename) for filename in dnn_files])\n",
    "# dnn_df.loc[:,'condition'] = 0\n",
    "# dnn_df['condition'] = dnn_df['condition'].astype(int)\n",
    "dnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df = pd.concat([pd.read_csv(filename) for filename in human_files])\n",
    "# human_df.loc[:,'filename'] = human_df.imagename.apply(lambda x: \"_\".join(x.split(\"_\")[-1:]))\n",
    "human_df.loc[:,'filename'] = human_df.imagename.apply(lambda x: \"_\".join(x.split(\"_\")[-2:]))\n",
    "human_df.loc[:,'is_correct'] = (human_df.object_response==human_df.category).astype(float)\n",
    "human_df.loc[:,'decision_margin'] = (human_df.object_response==human_df.category).astype(float)\n",
    "human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelvsmodel = compute_consistency(dnn_df)\n",
    "modelvsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelvsmodel.error_consistency.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelvsmodel.decision_margin_consistency.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=modelvsmodel.error_consistency,\n",
    "                     y=modelvsmodel.decision_margin_consistency)\n",
    "ax.axis('square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_split_corr, human_reliability = compute_reliability_from_halves(human_df)\n",
    "human_reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_split_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_df[human_df.subj=='subject-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_human_df = human_df.groupby(by=['filename']).mean(numeric_only=True).reset_index()\n",
    "avg_human_df.insert(0, 'subj', 'human_avg')\n",
    "if 'session' in avg_human_df.columns:\n",
    "    avg_human_df = avg_human_df.drop(columns=['session', 'trial'])\n",
    "else:\n",
    "    avg_human_df = avg_human_df.drop(columns=['trial'])\n",
    "avg_human_df.loc[:,'decision_margin'] = avg_human_df.loc[:,'is_correct']\n",
    "avg_human_df = avg_human_df.sort_values(by=\"filename\")\n",
    "avg_human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "results = defaultdict(list)\n",
    "subset1 = avg_human_df\n",
    "values1 = avg_human_df.is_correct.values.squeeze()\n",
    "for dnn in dnn_df.subj.unique():\n",
    "    subset2 = dnn_df[dnn_df.subj==dnn].sort_values(by=\"filename\")\n",
    "    assert (subset1.filename.values==subset2.filename.values).all()    \n",
    "    values2 = subset2.decision_margin.values.squeeze()\n",
    "    corr = pearsonr(values1, values2)[0]\n",
    "    results['dnn'].append(dnn)\n",
    "    results['decision_margin_consistency'].append(corr)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"decision_margin_consistency\", ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Adjust figure size\n",
    "plt.figure(figsize=(10, 6))  # Width: 10, Height: 6\n",
    "\n",
    "ax = sns.barplot(data=results_df, x=\"dnn\", y=\"decision_margin_consistency\")\n",
    "ax.axhline(y=human_reliability, color='gray', linestyle='-', linewidth=1.5)\n",
    "\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelvshuman = compute_consistency(dnn_df, human_df)\n",
    "modelvshuman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelvshuman_avg = modelvshuman.groupby(by=['sub1']).mean(numeric_only=True).reset_index()\n",
    "modelvshuman_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size\n",
    "plt.figure(figsize=(10, 6))  # Width: 10, Height: 6\n",
    "ax = sns.barplot(data=modelvshuman_avg, x=\"sub1\", y=\"error_consistency\")\n",
    "ax.set_ylim([0, .9])\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size\n",
    "plt.figure(figsize=(10, 6))  # Width: 10, Height: 6\n",
    "\n",
    "ax = sns.barplot(data=modelvshuman_avg, x=\"sub1\", y=\"decision_margin_consistency\")\n",
    "ax.axhline(y=human_reliability, color='gray', linestyle='-', linewidth=1.5)\n",
    "ax.set_ylim([0, .9])\n",
    "\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelvshuman",
   "language": "python",
   "name": "modelvshuman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
