{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# First look at the results\n",
    "\n",
    "This is just an initial preview of the results...let's see how decision-margin-consistency affects: (a) consitency scores (humanvshuman, modelvsmodel, and modelvshuman), and the rank-order of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# humanvshuman\n",
    "\n",
    "TLDR: agreement between humans is underestimated due to noise, and averaging across subjects increases the estimate of agreement between humans. Caveat that for these data, we can only make this conclusion for group-level agreement because we don't have multiple trials to average within subjects (though see other experiment for this case...)\n",
    "\n",
    "Here we compare error-consistency scores to accuracy-consistency scores, which can be taken as a measure of decision-margin consistency in people.\n",
    "\n",
    "Error consistency is computed by comparing the consistency of errors between individual subjects.\n",
    "\n",
    "Accuracy-consistency is computed by averaging across multiple trials, and is proportional to the decision margin distance for random sources of noise.\n",
    "\n",
    "In theory accuracy-consistency can be computed within individual subjects, if each item is presented multiple times. However, stimuli were presented only once to each subject in these experiments, and therefore we compute group-accuracy-consistency. \n",
    "\n",
    "To do so, we perform a split-half reliability analysis. First, we split the subjects into two groups, compute the average accuracy for each individual image separately for each group, then correlate the scores for each group across items. These split-half reliability scores were then adjusted to estimate the reliability of the full dataset using the Spearman-Browne adjustment, which we refer to here as the group-accuracy-consistency. This process repeated for all possible splits of subjects, and the average group-accuracy-consistency score is reported.\n",
    "\n",
    "Note that the \"group-accuracy-consistency\" score is a standard estimate of the \"noise ceiling\" for human behavioral data. No model is expected to correlate with these human behavioral data greater than this noise ceiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from modelvshuman_dmc import constants as c\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_humanvshuman_error_consistency_summary():\n",
    "    error_consistency = []\n",
    "    for dataset in c.DEFAULT_DATASETS:\n",
    "        datadir = f\"{c.RESULTS_DIR}/humanvshuman_error_consistency/{dataset}\"\n",
    "        filename = os.path.join(datadir, f\"humanvshuman_error_consistency_{dataset}_summary.csv\")\n",
    "        error_consistency.append(pd.read_csv(filename))\n",
    "\n",
    "    error_consistency = pd.concat(error_consistency)\n",
    "    \n",
    "    return error_consistency\n",
    "\n",
    "def load_humanvshuman_splithalves_noise_ceiling_summary():\n",
    "    noise_ceiling = []\n",
    "    for dataset in c.DEFAULT_DATASETS:\n",
    "        datadir = f\"{c.RESULTS_DIR}/humanvshuman_splithalves_noise_ceiling/{dataset}\"\n",
    "        filename = os.path.join(datadir, f\"humanvshuman_splithalves_noise_ceiling_{dataset}_summary.csv\")\n",
    "        noise_ceiling.append(pd.read_csv(filename))\n",
    "\n",
    "    noise_ceiling = pd.concat(noise_ceiling)\n",
    "    \n",
    "    return noise_ceiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls {c.RESULTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_consistency = load_humanvshuman_error_consistency_summary()\n",
    "error_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_ceiling = load_humanvshuman_splithalves_noise_ceiling_summary()\n",
    "noise_ceiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert (error_consistency.dataset == noise_ceiling.dataset).all(), \"Oops, better align your dfs\"\n",
    "assert (error_consistency.condition == noise_ceiling.condition).all(), \"Oops, better align your dfs\"\n",
    "\n",
    "# Select relevant columns and add 'metric' column to identify the source of the data\n",
    "error_df = error_consistency[['dataset', 'condition', 'error_consistency_avg']].copy()\n",
    "error_df['metric'] = 'error\\nconsistency'\n",
    "error_df.rename(columns={'error_consistency_avg': 'score'}, inplace=True)\n",
    "\n",
    "noise_df = noise_ceiling[['dataset', 'condition', 'adj_corr_mean']].copy()\n",
    "noise_df['metric'] = 'group-accuracy\\nconsistency'\n",
    "noise_df.rename(columns={'adj_corr_mean': 'score'}, inplace=True)\n",
    "\n",
    "# Concatenate the dataframes\n",
    "combined_df = pd.concat([error_df, noise_df], ignore_index=True)\n",
    "\n",
    "# Create the line plot\n",
    "plt.figure(figsize=(6, 8))\n",
    "ax = sns.lineplot(data=combined_df, x='metric', y='score', hue='dataset', style='condition', markers=True)\n",
    "\n",
    "# Remove style markers from the legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[:len(combined_df['dataset'].unique())], labels=labels[:len(combined_df['dataset'].unique())], bbox_to_anchor=(1.05, 1), \n",
    "          loc='upper left', borderaxespad=0, fontsize=13)\n",
    "\n",
    "plt.ylabel('score', fontsize=20, labelpad=16)\n",
    "ax.set_ylim([0,1.0])\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel('metric', fontsize=20, labelpad=16)\n",
    "ax.set_xlim([-.2,1.2])\n",
    "plt.xticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "df = noise_ceiling.copy()\n",
    "df['delta'] = noise_ceiling.adj_corr_mean - error_consistency.error_consistency_avg\n",
    "df = df.reset_index()\n",
    "\n",
    "ax = sns.scatterplot(data=df, x=\"adj_corr_mean\", y=\"delta\", hue=\"dataset\")\n",
    "ax.axis('square');\n",
    "\n",
    "# Remove style markers from the legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[:len(combined_df['dataset'].unique())], labels=labels[:len(combined_df['dataset'].unique())], bbox_to_anchor=(1.05, 1), \n",
    "          loc='upper left', borderaxespad=0, fontsize=13)\n",
    "\n",
    "plt.ylabel('score increase\\nrelative to error-consistency)', fontsize=16, labelpad=16)\n",
    "ax.set_ylim([-.1,1]);\n",
    "plt.yticks(fontsize=14);\n",
    "\n",
    "plt.xlabel('group-accuracy-consistency\\n(estimated noise ceiling)', fontsize=16, labelpad=16)\n",
    "ax.set_xlim([-.1,1]);\n",
    "plt.xticks(fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delta'].max(), df['delta'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# modelvsmodel\n",
    "\n",
    "Although the deep neural network models analyzed here are \"noiseless\", we find that agreement between models is increased for decision-margin consistency relative to error-consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from modelvshuman_dmc import constants as c\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_modelvsmodel_error_consistency(collection):\n",
    "    error_consistency = []\n",
    "    for dataset in c.DEFAULT_DATASETS:\n",
    "        datadir = f\"{c.RESULTS_DIR}/modelvsmodel_pairwise_error_consistency/{collection}/{dataset}\"\n",
    "        filename = os.path.join(datadir, f\"demo_set_modelvsmodel_pairwise_error_consistency_{dataset}.csv\")\n",
    "        error_consistency.append(pd.read_csv(filename))\n",
    "\n",
    "    error_consistency = pd.concat(error_consistency)\n",
    "    \n",
    "    return error_consistency\n",
    "\n",
    "def load_modelvsmodel_pairwise_decision_margin_consistency(collection):\n",
    "    dmc = []\n",
    "    for dataset in c.DEFAULT_DATASETS:\n",
    "        datadir = f\"{c.RESULTS_DIR}/modelvsmodel_pairwise_decision_margin_consistency/{collection}/{dataset}\"\n",
    "        filename = os.path.join(datadir, f\"{collection}_set_modelvsmodel_pairwise_decision_margin_consistency_{dataset}.csv\")\n",
    "        dmc.append(pd.read_csv(filename))\n",
    "\n",
    "    dmc = pd.concat(dmc)\n",
    "    dmc.rename(columns=dict(subject_A=\"sub1\", subject_B=\"sub2\"), inplace=True)\n",
    "    \n",
    "    return dmc\n",
    "\n",
    "def compute_heatmap(df, model_names, score_col):\n",
    "    N = len(model_names)\n",
    "    matrix = np.full((N, N), np.nan)\n",
    "    for model1,model2 in combinations(model_names, 2):\n",
    "        subset = df[(df.sub1==model1) & (df.sub2==model2)]\n",
    "        if len(subset) == 0:\n",
    "            subset = df[(df.sub2==model1) & (df.sub1==model2)]\n",
    "        assert len(subset) == 1, \"oops\"\n",
    "        idx1 = model_names.index(model1)\n",
    "        idx2 = model_names.index(model2)\n",
    "        matrix[idx1,idx2] = subset.iloc[0][score_col]\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def compute_heatmaps(df, score_col):\n",
    "    heatmaps = {}\n",
    "    model_names = np.unique(df.sub1.values.tolist() + df.sub2.values.tolist()).tolist()\n",
    "    datasets = df.dataset.unique()\n",
    "    for dataset in datasets:\n",
    "        subset = df[df.dataset == dataset]\n",
    "        conditions = subset.condition.unique()\n",
    "        for condition in conditions:\n",
    "            cond_df = subset[subset.condition==condition]\n",
    "            heatmaps[(dataset, condition)] = compute_heatmap(cond_df, model_names, score_col)\n",
    "    return heatmaps, model_names\n",
    "\n",
    "def plot_heatmap(matrix, model_names, vmin=0, vmax=1):\n",
    "    ax = sns.heatmap(matrix, vmin=vmin, vmax=vmax)\n",
    "    # Set x and y tick labels to model_names\n",
    "    ax.set_xticks(np.arange(len(model_names)) + 0.5);  # Center the tick marks\n",
    "    ax.set_yticks(np.arange(len(model_names)) + 0.5);\n",
    "    ax.set_xticklabels(model_names, rotation=90);\n",
    "    ax.set_yticklabels(model_names, rotation=0);\n",
    "    # Move x-tick labels to the top\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls {c.RESULTS_DIR}/modelvsmodel_pairwise_decision_margin_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_consistency = load_modelvsmodel_error_consistency(\"demo\")\n",
    "error_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmc = load_modelvsmodel_pairwise_decision_margin_consistency(\"demo\")\n",
    "dmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = error_consistency[(error_consistency.sub1=='') & (error_consistency.sub2=='')]\n",
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = error_consistency.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_consistency_heatmaps, err_con_model_names = compute_heatmaps(error_consistency, 'error_consistency')\n",
    "error_consistency_heatmaps[('edge', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_heatmap(error_consistency_heatmaps[('edge', 0)], err_con_model_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmc_heatmaps, dmc_model_names = compute_heatmaps(dmc, 'decision_margin_consistency')\n",
    "dmc_heatmaps[('edge', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_heatmap(dmc_heatmaps[('edge', 0)], dmc_model_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert err_con_model_names==dmc_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(error_consistency, dmc, on=['dataset','condition','sub1','sub2'], how='left')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(merged_df, x=\"error_consistency\", y=\"decision_margin_consistency\", hue=\"pair\")\n",
    "sns.lineplot(x=[-.1,1], y=[-.1,1], ax=ax, color=(.7,.7,.7), linestyle='--');\n",
    "ax.axis('square');\n",
    "\n",
    "ax.set_title(\"Error consistency vs. decision-margin consistency\\nall pairs of models across all datasets+conditions\", pad=20)\n",
    "# Remove style markers from the legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[:len(combined_df['dataset'].unique())], labels=labels[:len(combined_df['dataset'].unique())], bbox_to_anchor=(1.10, 1), \n",
    "          loc='upper left', borderaxespad=0, fontsize=13)\n",
    "\n",
    "plt.xlabel('error-consistency', fontsize=16, labelpad=16)\n",
    "ax.set_xlim([-.1,1]);\n",
    "plt.xticks(fontsize=14);\n",
    "\n",
    "plt.ylabel('decision-margin consistency', fontsize=16, labelpad=16)\n",
    "ax.set_ylim([-.1,1]);\n",
    "plt.yticks(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# modelvshuman\n",
    "\n",
    "Finally, what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modelvshuman_dmc import constants as c\n",
    "from modelvshuman_dmc.analysis import data\n",
    "from modelvshuman_dmc.datasets import experiments\n",
    "\n",
    "from pdb import set_trace\n",
    "\n",
    "def get_human_accuracy(datasets=c.DEFAULT_DATASETS):\n",
    "    results = []\n",
    "    for dataset in datasets:\n",
    "        df = data.load_human_data(f'{c.RAW_DATA_DIR}/{dataset}', expected_subjects=c.EXPECTED_SUBJECTS.get(dataset, 4))\n",
    "        drop_columns = [col for col in ['Session', 'session', 'trial'] if col in df.columns]\n",
    "        avg = df.groupby(by=['condition']).mean(numeric_only=True).drop(columns=drop_columns).reset_index()\n",
    "        \n",
    "        avg.insert(0, 'subj', \"humans\")\n",
    "        avg.insert(1, 'dataset_name', dataset)\n",
    "        avg.insert(2, 'metric_name', 'accuracy (top-1)')\n",
    "  \n",
    "        results.append(avg)\n",
    "    results = pd.concat(results)\n",
    "    return results\n",
    "\n",
    "def get_model_accuracy(model_names, datasets):\n",
    "    results = []\n",
    "    for model_name in model_names:\n",
    "        for dataset in datasets:\n",
    "            df = data.load_model_data(f'{c.RAW_DATA_DIR}/{dataset}', model_name)\n",
    "            drop_columns = [col for col in ['Session', 'session', 'trial', 'targ_act', 'max_nontarg_act', 'decision_margin'] if col in df.columns]\n",
    "            avg = df.groupby(by=['condition']).mean(numeric_only=True).drop(columns=drop_columns).reset_index()\n",
    "\n",
    "            avg.insert(0, 'subj', model_name)\n",
    "            avg.insert(1, 'dataset_name', dataset)\n",
    "            avg.insert(2, 'metric_name', 'accuracy (top-1)')\n",
    "            results.append(avg)\n",
    "    results = pd.concat(results)\n",
    "    return results\n",
    "\n",
    "def get_model_performance(model_names, dataset=None):\n",
    "    results = []\n",
    "    for model_name in model_names:\n",
    "        c.PERFORMANCES_DIR\n",
    "        filename = os.path.join(c.PERFORMANCES_DIR, f\"{model_name}.csv\")\n",
    "        df = pd.read_csv(filename)\n",
    "        if dataset is not None:\n",
    "            df = df[df.dataset_name==dataset]\n",
    "        results.append(df)\n",
    "    results = pd.concat(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls {c.RAW_DATA_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"alexnet\", \"resnet50\", \"bagnet33\", \"simclr_resnet50x1\", \"vit_b_16\", \"convnext_large\"]\n",
    "dataset = \"colour\"\n",
    "dataset = \"uniform-noise\"\n",
    "dataset = \"contrast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_acc = get_human_accuracy(datasets=[dataset])\n",
    "human_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = get_model_accuracy(model_names=models, datasets=[dataset])\n",
    "model_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = pd.concat([human_acc, model_acc])\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelvshuman_dmc import constants as c\n",
    "from modelvshuman_dmc.plotting.colors import *\n",
    "from modelvshuman_dmc.plotting.decision_makers import DecisionMaker\n",
    "\n",
    "__all__ = ['plotting_definition_template']\n",
    "\n",
    "def plotting_definition_template(df):\n",
    "    \"\"\"Decision makers to compare a few models with human observers.\n",
    "\n",
    "    This exemplary definition can be adapted for the\n",
    "    desired purpose, e.g., by adding more/different models.\n",
    "\n",
    "    Note that models will need to be evaluated first, before\n",
    "    their data can be plotted.\n",
    "\n",
    "    For each model, define:\n",
    "    - a color using rgb(42, 42, 42)\n",
    "    - a plotting symbol by setting marker;\n",
    "      a list of markers can be found here:\n",
    "      https://matplotlib.org/3.1.0/api/markers_api.html\n",
    "    \"\"\"\n",
    "\n",
    "    decision_makers = []\n",
    "\n",
    "    # Assign the blue color to alexnet\n",
    "    decision_makers.append(DecisionMaker(name_pattern=\"alexnet\",\n",
    "                           color=rgb(65, 90, 140), marker=\"o\", df=df,\n",
    "                           plotting_name=\"AlexNet\"))\n",
    "    \n",
    "    # New color for ResNet-50\n",
    "    decision_makers.append(DecisionMaker(name_pattern=\"resnet50\",\n",
    "                           color=rgb(120, 130, 190), marker=\"o\", df=df,\n",
    "                           plotting_name=\"ResNet-50\"))\n",
    "    \n",
    "    decision_makers.append(DecisionMaker(name_pattern=\"bagnet33\",\n",
    "                           color=rgb(110, 110, 110), marker=\"o\", df=df,\n",
    "                           plotting_name=\"BagNet-33\"))\n",
    "    \n",
    "    decision_makers.append(DecisionMaker(name_pattern=\"simclr_resnet50x1\",\n",
    "                           color=rgb(210, 150, 0), marker=\"o\", df=df,\n",
    "                           plotting_name=\"SimCLR-x1\"))\n",
    "    \n",
    "    # New color for ViT-B-16 (assigned a greenish hue)\n",
    "    decision_makers.append(DecisionMaker(name_pattern=\"vit_b_16\",\n",
    "                           color=rgb(0, 180, 100), marker=\"o\", df=df,\n",
    "                           plotting_name=\"ViT-B-16\"))\n",
    "    \n",
    "    # New color for ConvNeXt-Large (assigned a purple hue)\n",
    "    decision_makers.append(DecisionMaker(name_pattern=\"convnext_large\",\n",
    "                           color=rgb(150, 60, 200), marker=\"o\", df=df,\n",
    "                           plotting_name=\"ConvNeXt-Large\"))\n",
    "\n",
    "    decision_makers.append(DecisionMaker(name_pattern=\"humans\",\n",
    "                           color=rgb(165, 30, 55), marker=\"D\", df=df, markersize=10,\n",
    "                           plotting_name=\"Humans\"))\n",
    "    \n",
    "    return decision_makers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class PlotConfig:\n",
    "    \"\"\"\n",
    "    Plotting parameters\n",
    "    \"\"\"\n",
    "    ylabel: str\n",
    "    title: str\n",
    "    xlabel_fontsize: int = 16   \n",
    "    xlabel_labelpad: int = 10\n",
    "    ylabel_fontsize: int = 16   \n",
    "    ylabel_labelpad: int = 10\n",
    "    title_fontsize: int = 20\n",
    "    title_pad: int = 10\n",
    "    xticks_fontsize: int = 14\n",
    "    yticks_fontsize: int = 14\n",
    "    xlim: List[float] = field(default_factory=list)\n",
    "    ylim: List[float] = field(default_factory=list)\n",
    "    chance: Optional[float] = None\n",
    "    chance_label: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        assert True, \"How can we go wrong?\"\n",
    "\n",
    "accuracy_plot_cfg = PlotConfig(ylabel=\"Classification Accuracy\", \n",
    "                               xlabel_fontsize=16, xlabel_labelpad=10,\n",
    "                               ylabel_fontsize=16, ylabel_labelpad=10,\n",
    "                               title=\"Accuracy\", title_fontsize=20, title_pad=10,\n",
    "                               xticks_fontsize=14, yticks_fontsize=14,\n",
    "                               xlim=[-.15, 1.15], ylim=[0,1.0],\n",
    "                               chance=1/16, chance_label=None)\n",
    "plot_cfg = accuracy_plot_cfg\n",
    "plot_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot_cfg = PlotConfig(ylabel=\"Classification Accuracy\", \n",
    "                               xlabel_fontsize=16, xlabel_labelpad=10,\n",
    "                               ylabel_fontsize=16, ylabel_labelpad=10,\n",
    "                               title=\"Accuracy\", title_fontsize=20, title_pad=10,\n",
    "                               xticks_fontsize=14, yticks_fontsize=14,\n",
    "                               xlim=None, ylim=[0,1.0],\n",
    "                               chance=1/16, chance_label=None)\n",
    "plot_cfg = accuracy_plot_cfg\n",
    "plot_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\"alexnet\", \"resnet50\", \"bagnet33\", \"simclr_resnet50x1\", \"vit_b_16\", \"convnext_large\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_maker_fun = plotting_definition_template\n",
    "decision_makers = decision_maker_fun(acc_df)\n",
    "decision_makers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiments.__dict__[f'{dataset.replace(\"-\",\"_\")}_experiment']\n",
    "experiment, experiment.plotting_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTTING_EDGE_COLOR = (0.3, 0.3, 0.3, 0.3)\n",
    "PLOTTING_EDGE_WIDTH = 0.02\n",
    "\n",
    "def lineplot(df, decision_makers, experiment, plot_cfg):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    for decision_maker in decision_makers:\n",
    "        result_list = [df[(acc_df.subj==decision_maker.name_pattern[0]) & (df.condition==cond)].iloc[0].is_correct for cond in experiment.data_conditions]\n",
    "\n",
    "        plt.plot(experiment.plotting_conditions, result_list,\n",
    "                 marker=decision_maker.marker, color=decision_maker.color,\n",
    "                 markersize=decision_maker.markersize, linewidth=decision_maker.linewidth,\n",
    "                 markeredgecolor=PLOTTING_EDGE_COLOR,\n",
    "                 markeredgewidth=PLOTTING_EDGE_WIDTH, label=decision_maker.plotting_name)\n",
    "\n",
    "    # Add the chance line if plot_cfg.chance is not None\n",
    "    if plot_cfg.chance is not None:\n",
    "        plt.axhline(y=plot_cfg.chance, color='gray', linestyle='--', linewidth=1)\n",
    "        # Add text \"chance\" in italics, centered just above the line\n",
    "        if plot_cfg.chance_label is not None:\n",
    "            x_center = 0.5 * (ax.get_xlim()[0] + ax.get_xlim()[1])  # Calculate the midpoint of the x-axis\n",
    "            plt.text(x=x_center, y=plot_cfg.chance + 0.02, s='chance', color='gray', fontsize=12, style='italic',\n",
    "                     horizontalalignment='center')\n",
    "\n",
    "    # Add the legend and place it outside to the right\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(plot_cfg.ylim);\n",
    "    plt.yticks(fontsize=plot_cfg.yticks_fontsize);\n",
    "\n",
    "    if plot_cfg.xlim is not None:\n",
    "        ax.set_xlim(plot_cfg.xlim)\n",
    "    else:\n",
    "        xlim = ax.get_xlim()\n",
    "        ax.set_xlim([xlim[0]-.15, xlim[1]+.15])\n",
    "\n",
    "    plt.xticks(fontsize=plot_cfg.xticks_fontsize);\n",
    "\n",
    "    ax.set_title(plot_cfg.title, fontsize=plot_cfg.title_fontsize, pad=plot_cfg.title_pad)\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.set_xlabel(experiment.xlabel, fontsize=plot_cfg.xlabel_fontsize, labelpad=plot_cfg.xlabel_labelpad);\n",
    "    ax.set_ylabel(plot_cfg.ylabel, fontsize=plot_cfg.ylabel_fontsize, labelpad=plot_cfg.ylabel_labelpad);\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lineplot(acc_df, decision_makers, experiment, plot_cfg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelvshuman",
   "language": "python",
   "name": "modelvshuman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
