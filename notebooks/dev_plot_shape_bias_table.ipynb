{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from modelvshuman_dmc.datasets.experiments import get_experiments\n",
    "from modelvshuman_dmc.plotting.plot import get_dataset_names, rgb\n",
    "from modelvshuman_dmc.helper import plotting_helper as ph\n",
    "from modelvshuman_dmc.plotting import analyses as a\n",
    "from modelvshuman_dmc import constants as consts\n",
    "from examples.simclr_hn_models.plotting_definitions.simclr_hn_alexnets import plotting_definition_alexnets_simclr_hn as decision_maker_fun\n",
    "\n",
    "analysis = a.ShapeBias()\n",
    "datasets = get_experiments([\"cue-conflict\"])\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_maker_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(datasets) == 1\n",
    "ds = datasets[0]\n",
    "assert ds.name == \"cue-conflict\"\n",
    "\n",
    "df = ph.get_experimental_data(ds)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_maker_to_shape_bias_dict = {}\n",
    "colors = []\n",
    "labels = []\n",
    "label_colors = []\n",
    "for dmaker in decision_maker_fun(df):\n",
    "    if len(dmaker.decision_makers) > 1:\n",
    "        decision_maker_to_shape_bias_humans_dict = {}\n",
    "        for dmaker_human in dmaker.decision_makers:\n",
    "            df_selection = df.loc[(df[\"subj\"].isin([dmaker_human]))]\n",
    "            class_avgs = df_selection.groupby([\"category\"]).apply(lambda x: analysis.analysis(df=x)[\"shape-bias\"])\n",
    "            decision_maker_to_shape_bias_humans_dict[dmaker_human] = class_avgs.tolist()\n",
    "        df_results_humans = pd.DataFrame(decision_maker_to_shape_bias_humans_dict)\n",
    "        df_results_humans[\"humans\"] = df_results_humans.mean(axis=1)\n",
    "\n",
    "    else:\n",
    "        subject_name = dmaker.decision_makers[0]\n",
    "        df_selection = df.loc[(df[\"subj\"].isin(dmaker.decision_makers))]\n",
    "        class_avgs = df_selection.groupby([\"category\"]).apply(lambda x: analysis.analysis(df=x)[\"shape-bias\"])\n",
    "        decision_maker_to_shape_bias_dict[subject_name] = class_avgs.tolist()\n",
    "    colors.append(dmaker.color)\n",
    "    if subject_name in consts.TORCHVISION_MODELS:\n",
    "        label_colors.append(rgb(150, 150, 150))\n",
    "    else:\n",
    "        label_colors.append(dmaker.color)\n",
    "    labels.append(dmaker.plotting_name)\n",
    "\n",
    "decision_maker_to_shape_bias_dict[\"humans\"] = df_results_humans.humans.tolist()\n",
    "df_results = pd.DataFrame(decision_maker_to_shape_bias_dict)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# Calculate mean, lower, and upper 95% CI for each column\n",
    "result = []\n",
    "for col in df_results.columns:\n",
    "    mean = df_results[col].mean()\n",
    "    ci = st.t.interval(0.95, len(df_results[col])-1, loc=mean, scale=st.sem(df_results[col]))\n",
    "    result.append({\n",
    "        \"subj\": col,\n",
    "        \"mean\": mean,\n",
    "        \"lower_95_CI\": ci[0],\n",
    "        \"upper_95_CI\": ci[1]\n",
    "    })\n",
    "    \n",
    "summary_df = pd.DataFrame(result)\n",
    "# Add prop_human column based on the humans' mean value\n",
    "human_score = summary_df[summary_df['subj'] == 'humans']['mean'].values[0]\n",
    "summary_df['prop_human'] = summary_df['mean'] / human_score\n",
    "\n",
    "# Sort by mean, but ensure humans are first\n",
    "summary_df = summary_df.sort_values(by='mean', ascending=False)\n",
    "humans_row = summary_df[summary_df['subj'] == 'humans']\n",
    "summary_df = pd.concat([humans_row, summary_df[summary_df['subj'] != 'humans']])\n",
    "\n",
    "summary_df = summary_df.reset_index(drop=True)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def print_shape_bias_table(df):\n",
    "    # Remove humans row for ranking\n",
    "    df = copy.deepcopy(df[df[\"subj\"] != \"humans\"])\n",
    "    df['rank'] = df['mean'].rank(ascending=False)\n",
    "    df = df.sort_values(by=\"rank\", ascending=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Select and rename columns\n",
    "    df = df[[\"subj\", \"mean\", \"lower_95_CI\", \"upper_95_CI\", \"prop_human\", \"rank\"]]\n",
    "    df.rename(columns={\n",
    "        \"subj\": \"model\",\n",
    "        \"mean\": \"Mean\",\n",
    "        \"lower_95_CI\": \"Lower 95% CI\",\n",
    "        \"upper_95_CI\": \"Upper 95% CI\",\n",
    "        \"prop_human\": \"Prop. Human\",\n",
    "        \"rank\": \"Rank $\\\\downarrow$\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Escape LaTeX special characters\n",
    "    df[\"model\"] = df[\"model\"].apply(lambda x: x.replace(\"_\", \"\\\\_\"))\n",
    "\n",
    "    # Formatting\n",
    "    formatters = {}\n",
    "    cols_bold_mapping = {\"Mean\": max, \"Rank $\\\\downarrow$\": min}\n",
    "\n",
    "    def format_numbers(y, num_digits=3):\n",
    "        return f\"{y:.{num_digits}f}\"\n",
    "\n",
    "    for c, func in cols_bold_mapping.items():\n",
    "        m = func(df[c])\n",
    "        formatters[c] = lambda y, m=m: f\"\\\\textbf{{{format_numbers(y)}}}\" if y == m else format_numbers(y)\n",
    "\n",
    "    # Print LaTeX table\n",
    "    latex_table = df.to_latex(\n",
    "        escape=False,\n",
    "        formatters=formatters,\n",
    "        float_format=\"%.3f\",\n",
    "        index=False\n",
    "    )\n",
    "    print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shape_bias_table(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape_bias_table_with_humans(df):\n",
    "    # Separate humans row\n",
    "    humans_row = df[df[\"subj\"] == \"humans\"]\n",
    "    df = copy.deepcopy(df[df[\"subj\"] != \"humans\"])\n",
    "\n",
    "    # Rank and sort the rest of the data\n",
    "    df['rank'] = df['mean'].rank(ascending=False)\n",
    "    df = df.sort_values(by=\"rank\", ascending=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Add rank column for non-human rows only\n",
    "    humans_row['rank'] = None\n",
    "    df = pd.concat([humans_row, df], ignore_index=True)\n",
    "\n",
    "    # Select and rename columns\n",
    "    df = df[[\"subj\", \"mean\", \"lower_95_CI\", \"upper_95_CI\", \"prop_human\", \"rank\"]]\n",
    "    df.rename(columns={\n",
    "        \"subj\": \"model\",\n",
    "        \"mean\": \"Mean\",\n",
    "        \"lower_95_CI\": \"Lower 95\\\\% CI\",\n",
    "        \"upper_95_CI\": \"Upper 95\\\\% CI\",\n",
    "        \"prop_human\": \"Prop. Human\",\n",
    "        \"rank\": \"Rank $\\\\downarrow$\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Escape LaTeX special characters\n",
    "    df[\"model\"] = df[\"model\"].apply(lambda x: x.replace(\"_\", \"\\\\_\"))\n",
    "    df[\"Prop. Human\"] = df[\"Prop. Human\"].apply(lambda x: f\"{x:.3f}\".replace(\"%\", \"\\\\%\"))\n",
    "\n",
    "    # Formatting\n",
    "    formatters = {}\n",
    "\n",
    "    # Define the function to format numbers\n",
    "    def format_numbers(y, num_digits=3):\n",
    "        return f\"{y:.{num_digits}f}\"\n",
    "\n",
    "    # Determine the max value for the Mean column, excluding humans\n",
    "    max_mean_non_human = df[df[\"model\"] != \"humans\"][\"Mean\"].max()\n",
    "\n",
    "    # Define formatters for the Mean and Rank columns\n",
    "    def mean_formatter(value):\n",
    "        return f\"\\\\textbf{{{format_numbers(value)}}}\" if value == max_mean_non_human and not pd.isna(value) else format_numbers(value)\n",
    "\n",
    "    def rank_formatter(value):\n",
    "        return f\"\\\\textbf{{{format_numbers(value)}}}\" if value == 1 else format_numbers(value)\n",
    "\n",
    "    formatters[\"Mean\"] = mean_formatter\n",
    "    formatters[\"Rank $\\\\downarrow$\"] = rank_formatter\n",
    "\n",
    "    # Print LaTeX table\n",
    "    latex_table = df.to_latex(\n",
    "        escape=False,\n",
    "        formatters=formatters,\n",
    "        float_format=\"%.3f\",\n",
    "        index=False\n",
    "    )\n",
    "    print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shape_bias_table_with_humans(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelvshuman",
   "language": "python",
   "name": "modelvshuman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
