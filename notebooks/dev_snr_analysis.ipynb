{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# snr-consistency / decision-margin consistency (dmc)\n",
    "\n",
    "References\n",
    "\n",
    "- Spearman, C. (1910). Correlation calculated from faulty data. British Journal of Psychology, 3(3), 271-295.\n",
    "- Brown, W. (1910). Some experimental results in the correlation of mental abilities. British Journal of Psychology, 3(3), 296-322.\n",
    "- Nunnally, J. C., & Bernstein, I. H. (1994). Psychometric Theory (3rd ed.). McGraw-Hill. (See discussions on reliability estimation and the Spearman-Brown formula.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from itertools import combinations\n",
    "from scipy.stats import pearsonr\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_human_data(data_dir):\n",
    "    files = sorted(glob(os.path.join(data_dir, \"*subject-*\")))\n",
    "    df = None\n",
    "    for file in files:\n",
    "        df_ = pd.read_csv(file)\n",
    "        df_ = df_.sort_values(by='imagename')\n",
    "        df_['filename'] = df_.imagename.apply(lambda x: \"_\".join(x.split(\"_\")[-2:]))\n",
    "        df_['is_correct'] = (df_.object_response==df_.category).astype(float)\n",
    "        df = pd.concat([df, df_])\n",
    "        \n",
    "    return df\n",
    "\n",
    "def load_model_data(data_dir, model_name):\n",
    "    files = sorted(glob(os.path.join(data_dir, f\"*{model_name}*\")))\n",
    "    assert len(files)==1, f\"Expected one file, got {files}\"\n",
    "    file = files[0]\n",
    "    df_ = pd.read_csv(file)\n",
    "    df_ = df_.sort_values(by='imagename')\n",
    "    df_['filename'] = df_.imagename.apply(lambda x: \"_\".join(x.split(\"_\")[-2:]))\n",
    "    df_['is_correct'] = (df_.object_response==df_.category).astype(float)\n",
    "        \n",
    "    return df_\n",
    "\n",
    "def get_split_halves(N):\n",
    "    subjects = list(range(0,N))\n",
    "    splits = []\n",
    "    for subsetA in combinations(subjects, N//2):\n",
    "        subsetA = list(subsetA)\n",
    "        subsetB = list(np.setdiff1d(subjects, subsetA))\n",
    "        assert len(np.setdiff1d(subsetA,subsetB)) == len(subsetA), \"oops\"\n",
    "        assert len(np.setdiff1d(subsetB,subsetA)) == len(subsetB), \"oops\"\n",
    "        assert (len(subsetA) + len(subsetB)) == N, f\"oops, total should be {N}\"\n",
    "        splits.append((subsetA,subsetB))\n",
    "    \n",
    "    return splits[0:len(splits)//2] if N%2==0 else splits\n",
    "\n",
    "def error_consistency(expected_consistency, observed_consistency):\n",
    "        \"\"\"Return error consistency as measured by Cohen's kappa.\"\"\"\n",
    "\n",
    "        assert expected_consistency >= 0.0\n",
    "        assert expected_consistency <= 1.0\n",
    "        assert observed_consistency >= 0.0\n",
    "        assert observed_consistency <= 1.0\n",
    "\n",
    "        if observed_consistency == 1.0:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return (observed_consistency - expected_consistency) / (1.0 - expected_consistency)\n",
    "    \n",
    "def expected_consistency(df1, df2):\n",
    "    p1 = df1.is_correct.mean()\n",
    "    p2 = df2.is_correct.mean()\n",
    "    expected_consistency = p1 * p2 + (1 - p1) * (1 - p2)\n",
    "    \n",
    "    return expected_consistency, p1, p2\n",
    "\n",
    "def observed_consistency(df1, df2):\n",
    "    return (df1.is_correct == df2.is_correct).sum() / len(df1)\n",
    "\n",
    "def compute_error_consistency(df1, df2):\n",
    "    expected_con, p1, p2 = expected_consistency(df1, df2)\n",
    "    observed_con = observed_consistency(df1, df2)\n",
    "    error_con = error_consistency(expected_con, observed_con)\n",
    "    return expected_con, observed_con, error_con\n",
    "\n",
    "def compute_human_vs_model_error_consistency(human, model):\n",
    "    human_subjects = human.subj.unique()\n",
    "    human_cond = human.condition.unique()\n",
    "    model_subjects = model.subj.unique()\n",
    "    model_cond = model.condition.unique()\n",
    "    assert (human_cond == model_cond).all(), \"Human and Model data must contain the same conditions\"\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    for human_subj in human_subjects:        \n",
    "        for model_subj in model_subjects:\n",
    "            for condition in conditions:\n",
    "                df1 = human[(human.subj == human_subj) & (human.condition==condition)].sort_values(by='filename').reset_index(drop=True)\n",
    "                df2 = model[(model.subj == model_subj) & (model.condition==condition)].sort_values(by='filename').reset_index(drop=True)\n",
    "                expected_con, p1, p2 = expected_consistency(df1, df2)\n",
    "                observed_con = observed_consistency(df1, df2)\n",
    "                error_con = error_consistency(expected_con, observed_con)\n",
    "                \n",
    "                results['condition'].append(condition)\n",
    "                results['human_subj'].append(human_subj)\n",
    "                results['model_subj'].append(model_subj)\n",
    "                \n",
    "                results['human_pct_correct'].append(p1)\n",
    "                results['model_pct_correct'].append(p2)\n",
    "                \n",
    "                results['expected_consistency'].append(expected_con)\n",
    "                results['observed_consistency'].append(observed_con)\n",
    "                results['error_consistency'].append(error_con)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Colour vs. greyscale\n",
    "\n",
    "Number of human subjects is only 4 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.environ['MODELVSHUMANDIR'], 'raw-data', 'colour')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_human_data(data_dir)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = df.subj.unique()\n",
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = df[df.subj == subjects[0]]\n",
    "sub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1[sub1.imagename==\"0001_cl_s01_cr_oven_40_n04111531_14126.png\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = df[df.subj == subjects[1]]\n",
    "sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2[sub2.imagename==\"0001_cl_s01_cr_oven_40_n04111531_14126.png\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.imagename==\"0001_cl_s01_cr_oven_40_n04111531_14126.png\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'].value_counts().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = get_split_halves(len(subjects))\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "conditions = df.condition.unique()\n",
    "groupby = ['condition', 'filename']\n",
    "correlations = defaultdict(list)\n",
    "for split_num, (splitA,splitB) in enumerate(splits):\n",
    "    subA = subjects[splitA]\n",
    "    subB = subjects[splitB]\n",
    "    dfA = df[df.subj.isin(subA)]\n",
    "    dfB = df[df.subj.isin(subB)]\n",
    "    \n",
    "    grouped_A = dfA.groupby(groupby)['is_correct'].mean().reset_index()\n",
    "    grouped_A.rename(columns={'is_correct': 'mean_correct_A'}, inplace=True)\n",
    "    \n",
    "    grouped_B = dfB.groupby(groupby)['is_correct'].mean().reset_index()\n",
    "    grouped_B.rename(columns={'is_correct': 'mean_correct_B'}, inplace=True)\n",
    "    \n",
    "    merged_df = pd.merge(grouped_A, grouped_B, on=groupby, how='outer')\n",
    "    merged_df_sorted = merged_df.sort_values(by='filename').reset_index(drop=True)\n",
    "    \n",
    "    for condition in conditions:\n",
    "        cond_df = merged_df_sorted[merged_df_sorted.condition == condition]\n",
    "        r = pearsonr(cond_df.mean_correct_A, cond_df.mean_correct_B)[0]\n",
    "        correlations[condition].append(r)\n",
    "        results['split_num'].append(split_num)\n",
    "        results['splitA'].append(subA)\n",
    "        results['splitB'].append(subB)\n",
    "        results['condition'].append(condition)\n",
    "        results['pearsonr'].append(r)\n",
    "        \n",
    "for condition in conditions:\n",
    "    adjusted_correlations = [(2 * r) / (1 + r) for r in correlations[condition]]\n",
    "    avg_split_half_corr = np.mean(correlations[condition])\n",
    "    noise_ceiling = np.mean(adjusted_correlations)\n",
    "    print(condition, avg_split_half_corr, noise_ceiling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_summary = res_df.groupby(by='condition')['pearsonr'].mean().reset_index()\n",
    "res_summary.rename(columns={'pearsonr': 'avg_split_half_corr'}, inplace=True)\n",
    "r = res_summary['avg_split_half_corr']\n",
    "res_summary['noise_ceiling'] = (2 * r) / (1 + r)\n",
    "res_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "conditions = df.condition.unique()\n",
    "\n",
    "# compute error consistency\n",
    "num_subjects = len(subjects)\n",
    "for condition in conditions:\n",
    "    for idx1 in range(0,num_subjects-1):\n",
    "        sub1 = subjects[idx1]\n",
    "        df1 = df[(df.subj==sub1) & (df.condition==condition)]\n",
    "        df1 = df1.sort_values(by='filename').reset_index(drop=True)\n",
    "        for idx2 in range(idx1+1, num_subjects):\n",
    "            sub2 = subjects[idx2]\n",
    "            df2 = df[(df.subj==sub2)  & (df.condition==condition)]\n",
    "            df2 = df2.sort_values(by='filename').reset_index(drop=True)\n",
    "            (df1.filename == df2.filename).all(), \"Dataframe filenames not aligned\"\n",
    "\n",
    "            expected_con, p1, p2 = expected_consistency(df1, df2)\n",
    "            observed_con = observed_consistency(df1, df2)\n",
    "            error_con = error_consistency(expected_con, observed_con)\n",
    "            \n",
    "            results['condition'].append(condition)\n",
    "            results['subj1'].append(sub1)\n",
    "            results['subj2'].append(sub1)\n",
    "            results['pct_correct_subj1'].append(p1)\n",
    "            results['pct_correct_subj2'].append(p2)\n",
    "            results['expected_consistency'].append(expected_con)\n",
    "            results['observed_consistency'].append(observed_con)\n",
    "            results['error_consistency'].append(error_con)\n",
    "            \n",
    "error_con_df = pd.DataFrame(results)\n",
    "error_con_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_con_summary = error_con_df.groupby(by='condition')['error_consistency'].mean().reset_index()\n",
    "error_con_summary.rename(columns={'error_consistency': 'avg_error_consistency'}, inplace=True)\n",
    "error_con_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Next we need to reproduce the \"raw-data\" for models\n",
    "\n",
    "colour_vit-b-16_session-1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from modelvshuman.helper import wordnet_functions as wnf\n",
    "from modelvshuman.helper import human_categories as hc\n",
    "from modelvshuman.datasets.decision_mappings import DecisionMapping\n",
    "from pdb import set_trace\n",
    "\n",
    "class ResultAgg:\n",
    "    def __init__(self, model_name, dataset):\n",
    "        self.model_name = model_name\n",
    "        self.dataset = dataset\n",
    "        self.decision_mapping = self.dataset.decision_mapping\n",
    "        self.info_mapping = self.dataset.info_mapping\n",
    "        self.session_list = []\n",
    "        self.results = []\n",
    "        self.index = 0  # Initialize trial index\n",
    "\n",
    "    def print_batch(self, object_response, batch_targets, paths,\n",
    "                    target_act, max_non_target_act, decision_margin_act,\n",
    "                    target_prob, max_non_target_prob, decision_margin_prob):\n",
    "        \"\"\"\n",
    "        Aggregates batch results into the internal results list.\n",
    "\n",
    "        Parameters:\n",
    "        - object_response: List of model responses.\n",
    "        - batch_targets: List of target values (unused in this function but kept for compatibility).\n",
    "        - paths: List of file paths corresponding to each response.\n",
    "        - target_act: output activation (logit) for the target category\n",
    "        - max_non_target_act: max activation (logit) among non-target categories\n",
    "        - decision_margin_act: shorted distance from the (target_act, max_non_target_act) point to \n",
    "                           the decision boundary (unit line)\n",
    "        \"\"\"\n",
    "        for idx,(response, target, path) in enumerate(zip(object_response, batch_targets, paths)):\n",
    "            session_name, img_name, condition, category = self.info_mapping(path)\n",
    "            session_num = int(session_name.split(\"-\")[-1])\n",
    "\n",
    "            if session_num not in self.session_list:\n",
    "                self.session_list.append(session_num)\n",
    "                self.index = 0  # Reset index for new session\n",
    "\n",
    "            self.index += 1  # Increment trial index\n",
    "            \n",
    "            # Collect data into the results list\n",
    "            row = {\n",
    "                \"subj\": self.model_name,\n",
    "                \"session\": str(session_num),\n",
    "                \"trial\": str(self.index),\n",
    "                \"rt\": \"NaN\",  # Reaction time is not applicable here\n",
    "                \"object_response\": response[0],\n",
    "                \"category\": category,\n",
    "                \"condition\": condition,\n",
    "                \"imagename\": img_name,\n",
    "                \"filename\": \"_\".join(img_name.split(\"_\")[-2:]),\n",
    "                \"is_correct\": float(response[0] == category),\n",
    "                \"target_act\": target_act[idx],\n",
    "                \"max_non_target_act\": max_non_target_act[idx], \n",
    "                \"decision_margin_act\": decision_margin_act[idx],\n",
    "                \"target_prob\": target_prob[idx],\n",
    "                \"max_non_target_prob\": max_non_target_prob[idx], \n",
    "                \"decision_margin_prob\": decision_margin_prob[idx]\n",
    "            }\n",
    "            self.results.append(row)\n",
    "\n",
    "    def as_dataframe(self):\n",
    "        \"\"\"\n",
    "        Converts the aggregated results into a pandas DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: DataFrame containing the aggregated results.\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "class ImageNetProbabilitiesTo16ClassesMappingWithIndices(DecisionMapping):\n",
    "    \"\"\"Return the 16 class categories sorted by probabilities\"\"\"\n",
    "\n",
    "    def __init__(self, aggregation_function=None):\n",
    "        if aggregation_function is None:\n",
    "            aggregation_function = np.mean\n",
    "        self.aggregation_function = aggregation_function\n",
    "        self.categories = hc.get_human_object_recognition_categories()\n",
    "\n",
    "    def __call__(self, logits, probabilities):\n",
    "        # Ensure that logits and probabilities are valid and have matching shapes\n",
    "        self.check_input(probabilities)\n",
    "        assert logits.shape == probabilities.shape, \"Logits and probabilities must have the same shape.\"\n",
    "        \n",
    "        aggregated_class_probabilities = []\n",
    "        aggregated_class_logits = []\n",
    "        c = hc.HumanCategories()\n",
    "    \n",
    "        for category in self.categories:\n",
    "            indices = c.get_imagenet_indices_for_category(category)\n",
    "            # Aggregate probabilities\n",
    "            prob_values = np.take(probabilities, indices, axis=-1)\n",
    "            aggregated_prob = self.aggregation_function(prob_values, axis=-1)\n",
    "            aggregated_class_probabilities.append(aggregated_prob)\n",
    "            # Aggregate logits\n",
    "            logits_values = np.take(logits, indices, axis=-1)\n",
    "            aggregated_logit = self.aggregation_function(logits_values, axis=-1)\n",
    "            aggregated_class_logits.append(aggregated_logit)\n",
    "    \n",
    "        # Convert lists to arrays and transpose to shape (batch_size, 16)\n",
    "        aggregated_class_probabilities = np.array(aggregated_class_probabilities).T  # Shape: (batch_size, 16)\n",
    "        aggregated_class_logits = np.array(aggregated_class_logits).T  # Shape: (batch_size, 16)\n",
    "    \n",
    "        # Sort the aggregated probabilities to get sorted indices\n",
    "        sorted_indices = np.flip(np.argsort(aggregated_class_probabilities, axis=-1), axis=-1)  # Shape: (batch_size, 16)\n",
    "    \n",
    "        # Use sorted indices to sort categories, logits, and probabilities\n",
    "        sorted_categories = np.take(self.categories, sorted_indices, axis=-1)  # Shape: (batch_size, 16)\n",
    "        sorted_probs = np.take_along_axis(aggregated_class_probabilities, sorted_indices, axis=-1)  # Shape: (batch_size, 16)\n",
    "        sorted_logits = np.take_along_axis(aggregated_class_logits, sorted_indices, axis=-1)  # Shape: (batch_size, 16)\n",
    "        \n",
    "        return sorted_categories, sorted_logits, sorted_probs\n",
    "    \n",
    "class ImageNetActivationsTo16ClassesMappingWithIndices(DecisionMapping):\n",
    "    \"\"\"Return the 16 class categories sorted by probabilities\"\"\"\n",
    "\n",
    "    def __init__(self, aggregation_function=None):\n",
    "        if aggregation_function is None:\n",
    "            aggregation_function = np.mean\n",
    "        self.aggregation_function = aggregation_function\n",
    "        self.categories = hc.get_human_object_recognition_categories()\n",
    "\n",
    "    def __call__(self, logits, softmax):\n",
    "        aggregated_class_logits = []\n",
    "        c = hc.HumanCategories()\n",
    "    \n",
    "        for category in self.categories:\n",
    "            indices = c.get_imagenet_indices_for_category(category)\n",
    "            # Aggregate logits\n",
    "            logits_values = np.take(logits, indices, axis=-1)\n",
    "            aggregated_logit = self.aggregation_function(logits_values, axis=-1)\n",
    "            aggregated_class_logits.append(aggregated_logit)\n",
    "    \n",
    "        # Convert lists to arrays and transpose to shape (batch_size, 16)\n",
    "        aggregated_class_logits = np.array(aggregated_class_logits).T  # Shape: (batch_size, 16)\n",
    "        aggregated_class_probabilities = softmax(aggregated_class_logits)\n",
    "\n",
    "        # Sort the aggregated probabilities to get sorted indices\n",
    "        sorted_indices = np.flip(np.argsort(aggregated_class_probabilities, axis=-1), axis=-1)  # Shape: (batch_size, 16)\n",
    "    \n",
    "        # Use sorted indices to sort categories, logits, and probabilities\n",
    "        sorted_categories = np.take(self.categories, sorted_indices, axis=-1)  # Shape: (batch_size, 16)\n",
    "        sorted_probs = np.take_along_axis(aggregated_class_probabilities, sorted_indices, axis=-1)  # Shape: (batch_size, 16)\n",
    "        sorted_logits = np.take_along_axis(aggregated_class_logits, sorted_indices, axis=-1)  # Shape: (batch_size, 16)\n",
    "        \n",
    "        return sorted_categories, sorted_logits, sorted_probs\n",
    "    \n",
    "def compute_decision_margin_distance(act1, act2):\n",
    "    return signed_distance_to_unit_line(act1, act2)\n",
    "\n",
    "def signed_distance_to_unit_line(xi, yi):\n",
    "    '''Calculate the distance between the point (xi, yi) and the line x=y\n",
    "        distance point (x0,y0) to line (ax + by + c = 0):\n",
    "        abs(a * x0 + b * y0 + c) / sqrt(a^2 + b^2)\n",
    "        https://www.mathportal.org/calculators/analytic-geometry/line-point-distance.php\n",
    "    '''\n",
    "    distance = (xi-yi) / math.sqrt(2)\n",
    "    return distance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modelvshuman.utils import load_dataset, load_model\n",
    "\n",
    "def device():\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_analysis(model_name, dataset_name, num_workers=len(os.sched_getaffinity(0)), batch_size=32, use_logits=True):\n",
    "    data_dir = os.path.join(os.environ['MODELVSHUMANDIR'], 'raw-data', 'colour')\n",
    "    model, framework = load_model(model_name)\n",
    "    dataset = load_dataset(dataset_name, num_workers=num_workers, batch_size=batch_size)\n",
    "    results_agg = ResultAgg(model_name, dataset)\n",
    "    \n",
    "    if use_logits:\n",
    "        decision_mapping = ImageNetActivationsTo16ClassesMappingWithIndices(aggregation_function=np.mean)\n",
    "    else:\n",
    "        decision_mapping = ImageNetProbabilitiesTo16ClassesMappingWithIndices(aggregation_function=np.mean)\n",
    "    \n",
    "    for metric in dataset.metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    for images, target, paths in tqdm(dataset.loader):\n",
    "        bs = len(images)\n",
    "        images = images.to(device())\n",
    "        logits = model.forward_batch(images)\n",
    "        \n",
    "        if isinstance(target, torch.Tensor):\n",
    "            batch_targets = model.to_numpy(target)\n",
    "        else:\n",
    "            batch_targets = target\n",
    "            \n",
    "        if use_logits:\n",
    "            predictions, sorted_logits, sorted_probs = decision_mapping(logits, model.softmax)\n",
    "        else:\n",
    "            softmax_output = model.softmax(logits)\n",
    "            predictions = dataset.decision_mapping(softmax_output)\n",
    "            preds, sorted_logits, sorted_probs = decision_mapping(logits, softmax_output)\n",
    "            assert (preds==predictions).all()\n",
    "        \n",
    "        target_mask = predictions == np.array(batch_targets)[:, np.newaxis]\n",
    "        non_targets = ~target_mask\n",
    "        \n",
    "        target_act = sorted_logits[target_mask]\n",
    "        non_target_act = np.where(non_targets, sorted_logits, np.nan)\n",
    "        max_non_target_act = np.nanmax(non_target_act, axis=1)\n",
    "        decision_margin_act = compute_decision_margin_distance(target_act, max_non_target_act)\n",
    "\n",
    "        target_prob = sorted_probs[target_mask]\n",
    "        non_target_prob = np.where(non_targets, sorted_probs, np.nan)\n",
    "        max_non_target_prob = np.nanmax(non_target_prob, axis=1)\n",
    "        decision_margin_prob = compute_decision_margin_distance(target_prob, max_non_target_prob)\n",
    "\n",
    "        for metric in dataset.metrics:\n",
    "            metric.update(predictions,\n",
    "                          batch_targets,\n",
    "                          paths)\n",
    "\n",
    "        # Aggregate the batch results\n",
    "        results_agg.print_batch(predictions, \n",
    "                                batch_targets, \n",
    "                                paths,\n",
    "                                target_act,\n",
    "                                max_non_target_act,\n",
    "                                decision_margin_act,\n",
    "                                target_prob,\n",
    "                                max_non_target_prob,\n",
    "                                decision_margin_prob)\n",
    "        \n",
    "    return dataset, results_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"colour\"\n",
    "\n",
    "model_name = \"vit_b_16\"\n",
    "# model_name = \"alexnet2023_baseline_pgd\"\n",
    "# model_name = \"resnet18\"\n",
    "# model_name = \"resnet50\"\n",
    "# model_name = \"resnet50_l2_eps0_01\"\n",
    "dataset2, results_agg2 = run_analysis(model_name, dataset_name, num_workers=len(os.sched_getaffinity(0)), batch_size=32,\n",
    "                                      use_logits=False)\n",
    "\n",
    "for metric in dataset2.metrics:\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = results_agg2.as_dataframe()\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexnet\"\n",
    "# model_name = \"resnet34\"\n",
    "# model_name = \"resnet50_trained_on_SIN\"\n",
    "# model_name = \"resnet50_l2_eps0_03\"\n",
    "        # \"alexnet2023_baseline_pgd\",\n",
    "        # \"resnet50_l2_eps0\",\n",
    "        # \"resnet50_l2_eps0_01\",\n",
    "        # \"resnet50_l2_eps0_03\",\n",
    "        \n",
    "dataset_name = \"colour\"\n",
    "dataset3, results_agg3 = run_analysis(model_name, dataset_name, num_workers=len(os.sched_getaffinity(0)), batch_size=32,\n",
    "                                      use_logits=False)\n",
    "\n",
    "for metric in dataset3.metrics:\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = results_agg3.as_dataframe()\n",
    "results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_avg = df.groupby(by=['condition','filename'])['is_correct'].mean().reset_index()\n",
    "\n",
    "for condition in df.condition.unique():\n",
    "    human = human_avg[human_avg.condition==condition].sort_values(by='filename').reset_index(drop=True)\n",
    "    model1 = results2[results2.condition==condition].sort_values(by='filename').reset_index(drop=True)\n",
    "    model2 = results3[results3.condition==condition].sort_values(by='filename').reset_index(drop=True)\n",
    "    \n",
    "    corr1_act = pearsonr(human.is_correct, model1.decision_margin_act)[0]\n",
    "    corr1_prob = pearsonr(human.is_correct, model1.decision_margin_prob)[0]    \n",
    "    print(f\"model1vshuman, {condition}, {corr1_act:3.3f}, {corr1_prob:3.3f}\")\n",
    "    \n",
    "    corr2_act = pearsonr(human.is_correct, model2.decision_margin_act)[0]\n",
    "    corr2_prob = pearsonr(human.is_correct, model2.decision_margin_prob)[0]\n",
    "    print(f\"model2vshuman, {condition}, {corr2_act:3.3f}, {corr2_prob:3.3f}\")\n",
    "    \n",
    "    corr3_act = pearsonr(model1.decision_margin_act, model2.decision_margin_act)[0]\n",
    "    corr3_prob = pearsonr(model1.decision_margin_prob, model2.decision_margin_prob)[0]\n",
    "    print(f\"modelvsmodel, {condition}, {corr3_act:3.3f}, {corr3_prob:3.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_con1 = compute_human_vs_model_error_consistency(df, results2)\n",
    "err_con1.groupby(by=['condition'])['error_consistency'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_con1.groupby(by=['condition'])['human_pct_correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_con2 = compute_human_vs_model_error_consistency(df, results3)\n",
    "err_con2.groupby(by=['condition'])['error_consistency'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_con3 = compute_human_vs_model_error_consistency(results2, results3)\n",
    "err_con3.groupby(by=['condition'])['error_consistency'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.unique(), df.condition.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = [\n",
    "    dict(Color='color', correlation=0.7908335570106865, label=\"human-vs-human\"),\n",
    "    dict(Color='grayscale', correlation=0.7445981495245118, label=\"human-vs-human\"),\n",
    "  \n",
    "    dict(Color='color', correlation=0.071, label=\"vit-b-16 vs human\"),\n",
    "    dict(Color='grayscale', correlation=0.124, label=\"vit-b-16 vs human\"),\n",
    "    \n",
    "    dict(Color='color', correlation=0.232, label=\"alexnet vs human\"),\n",
    "    dict(Color='grayscale', correlation=0.253, label=\"alexnet vs human\"),\n",
    "    \n",
    "    dict(Color='color', correlation=0.664, label=\"model-vs-model\"),\n",
    "    dict(Color='grayscale', correlation=0.549, label=\"model-vs-model\"),\n",
    "]\n",
    "res = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Increased width for space on the right\n",
    "ax = sns.lineplot(data=res, x=\"Color\", y=\"correlation\", hue=\"label\")\n",
    "plt.legend(title='Day of Week', bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = [\n",
    "    dict(Color='color', correlation=0.7908335570106865, label=\"human-vs-human\"),\n",
    "    dict(Color='grayscale', correlation=0.7445981495245118, label=\"human-vs-human\"),\n",
    "  \n",
    "    dict(Color='color', correlation=0.080, label=\"vit-b-16 vs human\"),\n",
    "    dict(Color='grayscale', correlation=0.156, label=\"vit-b-16 vs human\"),\n",
    "    \n",
    "    dict(Color='color', correlation=0.337, label=\"alexnet vs human\"),\n",
    "    dict(Color='grayscale', correlation=0.366, label=\"alexnet vs human\"),\n",
    "    \n",
    "    dict(Color='color', correlation=0.504, label=\"model-vs-model\"),\n",
    "    dict(Color='grayscale', correlation=0.526, label=\"model-vs-model\"),\n",
    "]\n",
    "res = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Increased width for space on the right\n",
    "ax = sns.lineplot(data=res, x=\"Color\", y=\"correlation\", hue=\"label\")\n",
    "plt.legend(title='Day of Week', bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_avg.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "human.filename == model1.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vit-b-16\"\n",
    "data_dir = os.path.join(os.environ['MODELVSHUMANDIR'], 'raw-data', 'colour')\n",
    "results1 = load_model_data(data_dir, model_name)\n",
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vit_b_16\"\n",
    "model, framework = load_model(model_name)\n",
    "print(framework)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"colour\"\n",
    "dataset = load_dataset(dataset_name, num_workers=len(os.sched_getaffinity(0)), batch_size=32)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_agg = ResultAgg(model_name, dataset)\n",
    "results_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in dataset.metrics:\n",
    "    metric.reset()\n",
    "    \n",
    "decision_mapping = ImageNetActivationsTo16ClassesMappingWithIndices(aggregation_function=np.mean)\n",
    "        \n",
    "for images, target, paths in tqdm(dataset.loader):\n",
    "    bs = len(images)\n",
    "    images = images.to(device())\n",
    "    logits = model.forward_batch(images)\n",
    "    softmax_output = model.softmax(logits)\n",
    "    if isinstance(target, torch.Tensor):\n",
    "        batch_targets = model.to_numpy(target)\n",
    "    else:\n",
    "        batch_targets = target\n",
    "    # predictions = dataset.decision_mapping(softmax_output)\n",
    "    predictions, sorted_logits, sorted_probs = decision_mapping(logits, model.softmax)\n",
    "    \n",
    "    target_mask = predictions == np.array(batch_targets)[:, np.newaxis]\n",
    "    non_targets = ~target_mask\n",
    "    target_act = sorted_logits[target_mask]\n",
    "    non_target_act = np.where(non_targets, sorted_logits, np.nan)\n",
    "    max_non_target_act = np.nanmax(non_target_act, axis=1)\n",
    "    decision_margin_act = compute_decision_margin_distance(target_act, max_non_target_act)\n",
    "    \n",
    "    target_prob = sorted_probs[target_mask]\n",
    "    non_target_prob = np.where(non_targets, sorted_probs, np.nan)\n",
    "    max_non_target_prob = np.nanmax(non_target_prob, axis=1)\n",
    "    decision_margin_prob = compute_decision_margin_distance(target_prob, max_non_target_prob)\n",
    "    \n",
    "    for metric in dataset.metrics:\n",
    "        metric.update(predictions,\n",
    "                      batch_targets,\n",
    "                      paths)\n",
    "        \n",
    "    # Aggregate the batch results\n",
    "    results_agg.print_batch(predictions, \n",
    "                            batch_targets, \n",
    "                            paths,\n",
    "                            target_act,\n",
    "                            max_non_target_act,\n",
    "                            decision_margin_act,\n",
    "                            target_prob,\n",
    "                            max_non_target_prob,\n",
    "                            decision_margin_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in dataset.metrics:\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = results_agg.as_dataframe()\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.is_correct.sum(), (results2.decision_margin_prob>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "(results1.filename == results2.filename).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "(results1.is_correct == results2.is_correct).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_avg = df.groupby(by=['condition','filename'])['is_correct'].mean().reset_index()\n",
    "human_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_prob = results2.groupby(by=['condition','filename'])[['decision_margin_act', \n",
    "                                                         'decision_margin_prob']].mean().reset_index()\n",
    "dm_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "human = human_avg[human_avg.condition=='bw']\n",
    "human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_prob = dm_prob[dm_prob.condition=='bw']\n",
    "dm_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "(human.filename==dm_prob.filename).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human.is_correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(human.is_correct, dm_prob.decision_margin_prob)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(human.is_correct, dm_prob.decision_margin_act)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexnet\"\n",
    "model, framework = load_model(model_name)\n",
    "print(framework)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"colour\"\n",
    "dataset = load_dataset(dataset_name, num_workers=len(os.sched_getaffinity(0)), batch_size=32)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_agg = ResultAgg(model_name, dataset)\n",
    "results_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultAgg??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in dataset.metrics:\n",
    "    metric.reset()\n",
    "    \n",
    "decision_mapping = ImageNetProbabilitiesTo16ClassesMappingWithIndices(aggregation_function=np.mean)\n",
    "        \n",
    "for images, target, paths in tqdm(dataset.loader):\n",
    "    bs = len(images)\n",
    "    images = images.to(device())\n",
    "    logits = model.forward_batch(images)\n",
    "    softmax_output = model.softmax(logits)\n",
    "    if isinstance(target, torch.Tensor):\n",
    "        batch_targets = model.to_numpy(target)\n",
    "    else:\n",
    "        batch_targets = target\n",
    "    predictions = dataset.decision_mapping(softmax_output)\n",
    "    preds, sorted_logits, sorted_probs = decision_mapping(logits, logits)\n",
    "    # assert (preds == predictions).all(), \"oops, predictions are wrongo\"\n",
    "    \n",
    "    target_mask = preds == np.array(batch_targets)[:, np.newaxis]\n",
    "    non_targets = ~target_mask\n",
    "    target_act = sorted_logits[target_mask]\n",
    "    non_target_act = np.where(non_targets, sorted_logits, np.nan)\n",
    "    max_non_target_act = np.nanmax(non_target_act, axis=1)\n",
    "    decision_margin_act = compute_decision_margin_distance(target_act, max_non_target_act)\n",
    "    \n",
    "    target_prob = sorted_probs[target_mask]\n",
    "    non_target_prob = np.where(non_targets, sorted_probs, np.nan)\n",
    "    max_non_target_prob = np.nanmax(non_target_prob, axis=1)\n",
    "    decision_margin_prob = compute_decision_margin_distance(target_prob, max_non_target_prob)\n",
    "    \n",
    "    for metric in dataset.metrics:\n",
    "        metric.update(predictions,\n",
    "                      batch_targets,\n",
    "                      paths)\n",
    "        \n",
    "    # Aggregate the batch results\n",
    "    results_agg.print_batch(preds, \n",
    "                            batch_targets, \n",
    "                            paths,\n",
    "                            target_act,\n",
    "                            max_non_target_act,\n",
    "                            decision_margin_act,\n",
    "                            target_prob,\n",
    "                            max_non_target_prob,\n",
    "                            decision_margin_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = results_agg.as_dataframe()\n",
    "results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_prob3 = results3.groupby(by=['condition','filename'])[['decision_margin_act', \n",
    "                                                         'decision_margin_prob']].mean().reset_index()\n",
    "dm_prob3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_prob3 = dm_prob3[dm_prob3.condition=='bw']\n",
    "dm_prob3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "(human.filename==dm_prob3.filename).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(human.is_correct, dm_prob3.decision_margin_prob)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(human.is_correct, dm_prob3.decision_margin_act)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(dm_prob.decision_margin_act, dm_prob3.decision_margin_act)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=dm_prob.decision_margin_act, y=dm_prob3.decision_margin_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=human.is_correct, y=dm_prob3.decision_margin_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelvshuman",
   "language": "python",
   "name": "modelvshuman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
